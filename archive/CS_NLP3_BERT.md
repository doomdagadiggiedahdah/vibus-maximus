# BERT Models

Bidirectional Encoder Representations from Transformers:
- Pre-trained on massive text corpora
- Contextual word representations
- Fine-tuned for specific tasks like classification and QA